# 【笔记及总结】

## 数学基础

### [**协方差Covariance**](https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE)

1.  用于衡量随机变量间的相关程度。
    
2.  定义：
    

设${\displaystyle \Omega }$为样本空间，${\displaystyle P}$是定义在${\displaystyle \Omega }$的事件族 ${\displaystyle \Sigma }$上的概率。（换句话说，${\displaystyle (\Omega ,\,\Sigma ,\,P)}$是个概率空间）

若 ${\displaystyle X}$与 ${\displaystyle Y}$是定义在${\displaystyle \Omega }$上的两个实数随机变量， 期望值分别为：

${\displaystyle \operatorname {E} (X)=\int \_{\Omega }X\,dP=\mu }$

${\displaystyle \operatorname {E} (Y)=\int \_{\Omega }Y\,dP=\nu }$

则两者间的**协方差**定义为：

${\displaystyle \operatorname {cov} (X,Y)=\operatorname {E} [(X-\mu )(Y-\nu ) ]}$

3.  根据测度积分的线性性质，上面的原始定义可以进一步简化为：
    

${\displaystyle {\begin{aligned}\operatorname {cov} (X,Y)&=\int \_{\Omega }(X-\mu )(Y-\nu )\,dP\\&=\int \_{\Omega }X\cdot Y\,dP-\mu \int \_{\Omega }Y\,dP-\nu \int \_{\Omega }X\,dP+\mu \nu \\&=\operatorname {E} (X\cdot Y)-\mu \nu \end{aligned}}}$

4.  协方差矩阵
    

*   协方差的定义可以推广到两列随机变量之间
    
*   定义
    

设${\displaystyle (\Omega ,\,\Sigma ,\,P)}$是概率空间，${\displaystyle X=\{x\_{i}\}\_{i=1}^{m}}$与${\displaystyle Y=\{y\_{j}\}\_{j=1}^{n}}$是定义在${\displaystyle \Omega }$上的两列实数随机变量序列（也可视为有序对或行向量）

若二者对应的期望值分别为：

${\displaystyle E(x\_{i})=\int \_{\Omega }x\_{i}\,dP=\mu \_{i}}$

${\displaystyle E(y\_{j})=\int \_{\Omega }y\_{j}\,dP=\nu \_{j}}$

则这两列随机变量间的**协方差**定义成一个${\displaystyle m\times n}$矩阵

${\displaystyle \operatorname {\mathbf {cov} } (X,Y):={\left [\,\operatorname {cov} (x\_{i},y\_{j})\,\right]}\_{m\times n}}$

以上的定义，以矩形来表示就是：

${\displaystyle \operatorname {\mathbf {cov} } (X,Y):={\begin{bmatrix}\operatorname {cov} (x\_{1},y\_{1})&\dots &\operatorname {cov} (x\_{1},y\_{n})\\\vdots &\ddots &\vdots \\\operatorname {cov} (x\_{m},y\_{1})&\dots &\operatorname {cov} (x\_{m},y\_{n})\end{bmatrix}}={\begin{bmatrix}\operatorname {E} (x\_{1}y\_{1})-\mu \_{1}\nu \_{1}&\dots &\operatorname {E} (x\_{1}y\_{n})-\mu \_{1}\nu \_{n}\\\vdots &\ddots &\vdots \\\operatorname {E} (x\_{m}y\_{1})-\mu \_{m}\nu \_{1}&\dots &\operatorname {E} (x\_{m}y\_{n})-\mu \_{m}\nu \_{n}\end{bmatrix}}}$

### [雅可比矩阵](https://zh.wikipedia.org/wiki/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5)

1.  雅可比矩阵（Jacobian matrix）：函数的一阶偏导数以一定的方式排列成的矩阵；
    
2.  雅可比行列式（Jacobian determinant）：方形雅可比矩阵。
    
3.  其重要性在于，如果函数  **f** : ℝ^_n_^ → ℝ^_m_^ 在点 **x** 可微的话，在点 **x** 的雅可比矩阵即为**该函数在该点的最佳线性逼近**，也代表雅可比矩阵是**单变数实数函数的微分在向量值多变数函数的推广**，在这种情况下，雅可比矩阵也被称作函数 **f** 在点 **x** 的微分或者导数。
    
4.  定义：
    
    1.  假设某函数从$f : ℝ^n → ℝ^m$， 从$x ∈ ℝ^n$映射到 向量$f(x) ∈ ℝ^m$， 其雅可比矩阵是一$m×n$的矩阵，换句话讲也就是从$ℝ^n$到$ℝ^m$的线性映射，其重要意义在于它**表现了一个多变数向量函数的最佳线性逼近**。
        
    2.  此函数 f 的雅可比矩阵 J 为$m×n$的矩阵，一般由以下方式定义：
        

${\displaystyle \mathbf {J}  ={\begin{bmatrix}{\dfrac {\partial \mathbf {f} }{\partial x\_{1}}}&\cdots &{\dfrac {\partial \mathbf {f} }{\partial x\_{n}}}\end{bmatrix}} ={\begin{bmatrix}{\dfrac {\partial f\_{1}}{\partial x\_{1}}}&\cdots &{\dfrac {\partial f\_{1}}{\partial x\_{n}}}\\\vdots &\ddots &\vdots \\{\dfrac {\partial f\_{m}}{\partial x\_{1}}}&\cdots &{\dfrac {\partial f\_{m}}{\partial x\_{n}}}\end{bmatrix}}}$

矩阵的分量可表示成：

${\displaystyle \mathbf {J} \_{ij}={\frac {\partial f\_{i}}{\partial x\_{j}}}}$

雅可比矩阵的其他常用符号还有：

${\displaystyle Df}、 D f {\displaystyle \mathrm {D} \mathbf {f} }、 J f ( x 1 , … , x n ) {\displaystyle \mathbf {J} \_{\mathbf {f} }(x\_{1},\ldots ,x\_{n})}$或者 ${\displaystyle {\frac {\partial (f\_{1},\ldots ,f\_{m})}{\partial (x\_{1},\ldots ,x\_{n})}}}$

此矩阵的第${\displaystyle i}$行是由函数${\displaystyle f\_{i}}$的梯度函数所表示的， ${\displaystyle 1\leq i\leq m}$。

如果${\displaystyle p}$是${\displaystyle \mathbb {R} ^{n}}$中的一点， f {\displaystyle f}在${\displaystyle p}$点可微分，根据数学分析，${\displaystyle \mathbf {J} \_{\mathbf {f} }(p)}$是在这点的导数。**在此情况下，**${\displaystyle \mathbf {J} \_{\mathbf {f} }(p)}$**这个线性映射即**${\displaystyle f}$**在点**${\displaystyle p}$**附近的最优线性逼近，也就是说当**${\displaystyle x}$**足够靠近点**${\displaystyle p}$**时，我们有**

${\displaystyle f(x)\approx f(p)+\mathbf {J} \_{\mathbf {f} }(p)\cdot (x-p)}$

讲更详细点也就是：

 ${\displaystyle \mathbf {f} (\mathbf {x} )=\mathbf {f} (\mathbf {p} )+\mathbf {J} \_{\mathbf {f} }(\mathbf {p} )(\mathbf {x} -\mathbf {p} )+o(\|\mathbf {x} -\mathbf {p} \|)}$

其中，$o$代表小o符号，$‖x − p‖$为 x 与 p 之间的距离。 

## 零、基础知识及理论

[隐马尔可夫模型](https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B)

1.  Hidden Markov Model；HMM。是一种统计模型，用来描述一个**含有隐含未知参数**的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。
    
2.  区别与联系
    
    1.  **正常**的**马尔可夫模型**中，**状态**对于观察者来说是**直接可见**的。这样状态的转换概率便是全部的参数。
        
    2.  在**隐马尔可夫**中，状态并**不是直接可见**的，，但受状态影响的**某些变量则是可见**的。每一个状态在可能输出的符号上都有一**概率分布**。
        
3.  模型的概率
    

1.  假设观察到的结果为Y
    

$Y=y(0),y(1),...,y(L−1)$

1.  隐藏条件为X
    

$X=x(0),x(1),...,x(L−1)$

2.  长度为L，则马尔可夫模型的概率可以表达为：
    

$P(Y)=∑XP(Y∣X)P(X)$

4.  结构
    

1.  实例化的一般结构
    

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/8f7fa5d0-da7e-4cc5-834d-7b2c3da0e903.png)

1.  从图中可知：给定隐变量$x(t)$在时间$t$的**条件概率分布****只取决于隐变量**$x(t−1)$**的值**，之前的则没有影响，这就是所谓马尔可夫性质。观测变量$y(t)$同理，只取决于隐变量$x(t)$的值。
    

## 一、机器学习相关

**栾生神经网络**

[https://zhuanlan.zhihu.com/p/35040994](https://zhuanlan.zhihu.com/p/35040994)

[三方应用: https://zhuanlan.zhihu.com/p/677110509](https://zhuanlan.zhihu.com/p/677110509)

1.  **要点**
    
    1.  两个共享权值的神经网络，甚至可以是同一个网络
        
    2.  分别将输入映射到新的空间，形成输入在新的空间中的表示
        
    3.  通过loss计算，评价两个输入的相似度
        
2.  适用场景
    
    1.  孪生神经网络：两个输入**比较类似**
        
    2.  伪孪生神经网络：两个输入**有一定差别**
        
3.   用途
    
    1.  词汇的语义相似度
        
    2.  **手写体识别**
        
        1.  [Arabic-handwritten-one-shot-learning](https://github.com/HusseinYoussef/Arabic-handwritten-one-shot-learning)
            
        2.  [Signature verification system using Siamese networks](https://github.com/seanbenhur/siamese_net)
            
    3.  **视觉跟踪算法**
        

[filter 与 kernel ,卷积的理解](https://www.cnblogs.com/chumingqian/articles/11569397.html)

1.  filter与kernel的区别与联系
    
    1.  kernel倾向于是二维的权重矩阵。
        
    2.  filter可以理解为多个kernel堆叠的结构。
        
    3.  ![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/e9e7d8fd-759e-4bdc-9317-404400e452d8.png)
        
2.  二维卷积与三维卷积
    
    1.  二维卷积：3D Filter只需要在二维方向上移动，即图像的高和宽；此时通道数量=单个filter中卷积核数量。
        
    2.  三维卷积：3D Filter需要在三个维度方向上移动（图像长、宽、高）；此时滤波器深度小于输入层深度，即卷积核数量小于通道数量。
        
    3.  下图左右分别为二维卷积和三维卷积。
        

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/c23b1330-a2ea-4d72-b1ab-111e2701ac7f.png)         ![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/c5d278a6-d475-4f4a-82d1-77d2b70c3f9e.png)

3.  卷积与互相关；convolution vs cross-correlation
    
    1.  深度学习中的卷积中的滤波器不翻转，基本上执行的是元素对元素的乘法或者加法。严格来说，不是信号处理中的卷积。滤波器的权重在训练期间学习获得。
        
    2.  比较convolution, [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation), and [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation).
        

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/09f92e96-2e1d-4170-8906-370556baa695.png)

3.  卷积：滤波器g需要首先翻转，然后沿着横坐标移动，计算两者相交的面积，即卷积值。
    

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/1119ede1-f54f-45f1-a4d0-194d43bf8f4a.png)

4.  互相关：称为滑动点积或者两个函数的滑动内积；滤波器函数不用翻转；直接划过特征函数f，f和g相交的区域就是Cross-correlation。
    
5.  自相关/序列相关：一个信号与其自身在不同时间点的相关性。
    
    1.  统计学上，指两个随机过程中不同时刻的数值之间的皮尔森相关(Pearson correlation).
        

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/b0c92090-724e-47fb-bc43-035b423a8138.png)

2.  信号处理中，同一时间函数在瞬时t和t+a的两个值相乘积的平[均值](https://baike.baidu.com/item/均值/0?fromModule=lemma_inlink)作为延迟时间t的函数，它是信号与延迟后信号之间相似性的度量。延迟时间为零时，则成为信号的均方值，此时它的值最大。
    

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/0f44c259-531b-4b32-83ba-2f3a5e224c3a.png)

**KAN: Kolmogorov-Arnold Networks**

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/0e777bd2-37db-4974-b863-715ccb254196.png)

1.  与Multi-Layer Perceptron (MLPs)的区别
    
    1.  MLPs: 在节点/神经元（neurons）上有**固定**的激活函数
        
    2.  KANs: 在边缘（weights）上有**可学习**的激活函数
        
        1.  没有线性权重
            
        2.  每个权重值由单变量函数参数化的样条曲线取代
            
2.  优势（想对于MLPs）
    
    1.  在精度和可解释性上优于MLPs
        
    2.  更小的KANs可实现相当或者优于MLPs的精度（数据拟合、偏微分方程求解）
        
    3.  具有更快的neural scaling laws
        
    4.  直接可视化、交互
        
3.  理论来源
    
    1.  MLPs: universal approximation  theorem
        
    2.  KANs: Kolmogorov-Arnold representation theorem
        
    3.  KANs是MLPs on the outside and splines on the inside.
        
4.  **数学理论**
    
    1.  **Kolmogorov-Arnold Theorem (2.1)**
        
        1.  如果$f$为有界域上的多变量连续函数，那么$f$可写成**单变量连续函数**和**加法**的二元运算的有限组合。
            
        2.  具体地，对于平滑的$f : \[0, 1\]^n → \R$
            

$f (x) = f (x\_1, · · · , x\_n) = \sum^{2n+1}\_{q=1} Φ\_q \Bigg( \sum^{n}\_{p=1} \phi\_{q,p}(x\_p) \Bigg)$

且有：$\phi\_{q,p} : \[0, 1\] → \R$和$Φ\_q : \R → \R$

1.   问题：可能是不平滑，甚至碎片化，因此实践中无法进行学习。
    
    2.  解决方式：
        
        1.  不必拘泥于原公式中的两层非线性和少量（$2n+1$）元素； 
            
        2.  多数函数通常是平滑且具有稀疏的组成结构。
            

1.  **KAN架构**
    

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/03bf8fef-5bc0-4432-9a43-78c64d238813.png)

2.  **KAN’s Approximation Abilities and Scaling Laws (2.3)**
    
3.  **准确性**
    
4.  **Methodology: Grid extension (2.4)**
    
    1.  理论上，样条曲线可以以任意程度接近目标函数，因为网格可以实现任意细粒度。KANs继承了该特性。
        
    2.  可以先训练一个参数较少的KAN，然后通过简单的**细化样条曲线网格**扩展KAN的参数量，而无需从头开始训练大模型。
        
    3.  实现方式：（图2.2右）
        
        1.  假设要在区间$\[a,b\]$内$k$阶B样条曲线逼近一维函数$f$；
            
        2.  一个有$G\_1$个区间粗颗粒网格有网格点$\{t\_0 = a, t\_1, t\_2, · · · , t\_{G1} = b\}$，增加到$\{t\_{−k}, · · · , t\_{-1}, t\_0, · · · , t\_{G1} , t\_{G1+1}, · · · , t\_{G1+k}\}$；
            
        3.  共有$G\_1 +k$个B样条基函数，其中第$i^{th}$B样条$B\_i(x)$仅在区间$\[t\_{-k+i}, t\_{i+1}\]$$(i =0, ..., G\_1+k-1)$非零；
            
        4.  函数$f$在粗粒网格由这些B样条曲线基函数的线性组合进行表达：
            

$f\_{coarse}(x) = \sum^{G\_1+k−1}\_{i=0} c\_iB\_i(x)$

5.  若给定有$G\_2$个区间的细粒网格，函数$f$表达为：
    

$f\_{fine}(x) = \sum^{G\_2+k−1}\_{=0} c'\_jB'\_j(x)$

6.  通过最小化$f\_{fine}(x)$和$f\_{coarse}(x)$距离，可由参数$c\_i$来初始化$c'\_j$：
    

$\{c'\_j\} = \underset {\{c'\_j\}} {argmin} \underset{x∼p(x)}{\Bbb{E}} \Bigg ( \sum\_{ j=0}^{G\_2 +k−1}c'\_jB'\_j(x)- \sum\_{ I=0}^{G\_1 +k−1}c\_IB\_I(x) \Bigg )^2$

可用最小二乘法实现。

1.  阶梯形损失曲线
    
    1.  插值阈值interpolation threshold
        

$G= Num\_{training samples}/ Num\_{grid intervals}$

1.  参数量小的KANs泛化性更好
    
    1.  参数量跟性能没有直接的关系，因此需要谨慎选择KAN的结构。
        
    2.  Scaling law：与理论的比较
        

1.  对于数据拟合以及解偏微分方程问题，KANs比MLPs具有显著更好的scaling law。
    

8.  内外自由度
    
    1.  外部自由度：节点如何连接；负责学习多变量的组成结构
        
        2.  内部自由度：激活函数内部的网格节点；负责学习单变量函数
            
        3.  KANs同时具有外部和内部自由度
            

1.  **Application: Data fitting, PDE (3)**
    
2.  **可解释性**
    
3.  **Methodology: Simplification (2.5)**
    

**从足够大的KAN开始，然后以稀疏正则化来训练，最后进行剪枝。**

1.  **简化技术**
    

1.  稀疏化
    
    1.  在MLPs中，L1正则化在KANs中应用时需要做两点修改
        
        1.  需定义可学习激活函数的L1模值；
            
            2.  L1不足以稀疏化KANs，需要另外的entropy regularization。
                
            3.  L1模值
                
            4.  熵
                
            5.  损失函数
                
        2.  可视化：较小的函数具有更高的透明度。
            
        3.  剪枝
            
            1.  在节点层级（而不是边缘层级）进行稀疏化；
                
            2.  定义每个节点的传入和传出分数
                

$I\_{l,i} =\underset{k} {max} (|\phi \_{l−1,k,i}|\_1), \quad O\_{l,i} = \underset{j}{max} (|\phi \_{l+1,j,i}|\_1)$

3.  如果两个分数均高于阈值（$\theta = 10^{-2}$），则为重要节点；
    
    4.  修剪掉所有非重要节点。
        
        4.  **符号化**
            
        
4.  **Application: AI for math & physics (4)**
    

**Neural Network on Microcontroller (NNoM)**

仓库地址：[https://github.com/majianjia/nnom](https://github.com/majianjia/nnom)

使用教程：[https://majianjia.github.io/nnom/](https://majianjia.github.io/nnom/)

**Highlights**

*   Deploy **Keras model** to NNoM model with one line of code.
    
*   Support complex structures; Inception, ResNet, DenseNet, Octave Convolution...
    
*   User-friendly interfaces.
    
*   High-performance backend selections.
    
*   Onboard pre-compiling - zero interpreter performance loss at runtime.
    
*   Onboard evaluation tools; Runtime analysis, Top-k, Confusion matrix...
    

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/Mp7ldV1m87ZAqBQN/img/9fdc1f21-320f-4861-b4a5-a68f5d608264.png)

## 二、计算机视觉/图像处理相关

**极线校正**

[https://docs.opencv.org/4.x/d9/d0c/group\_\_calib3d.html](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html)

[https://zhuanlan.zhihu.com/p/348846552](https://zhuanlan.zhihu.com/p/348846552)

[https://blog.csdn.net/qq\_30234963/article/details/121908465](https://blog.csdn.net/qq_30234963/article/details/121908465)

1.  目的
    
    1.  已知左右双目相机的外参矩阵（$R\_{lr}$和$T\_{lr}$）
        
    2.  将左右相机的坐标系旋转到平行且共面
        
2.  实现方式
    
    1.  左右相机各自旋转一半，此时左右相机坐标系各面**平行但不共面**
        
        1.  若对应的旋转向量$rot\_{lr}=\theta \*u$，其中$\theta$为旋转角度，$u$为单位向量
            
        2.  Bouguet双目立体校正中，左相机坐标系旋转一半的$R\_{lr}$ ，对应到各轴则是$0.5 \* rot\_{lr}$；右相机坐标系反方向旋转另外一半的$R\_{lr}$ ，对应到各轴则是$-0.5 \* rot\_{lr}$
            
    2.  旋转到平行且共面
        
        1.  矫正坐标系与相机坐标系的关系
            
            1.  $O\_{rec}X\_{rec}$ 轴与左右相机坐标系原点连线$O\_{L}X\_{R}$ 平行，正方向由$O\_{L}$ 指向$O\_{R}$ 
                
            2.  矫正坐标系的$X\_{rec}O\_{rec}Z\_{rec}$ 平面与平面$Z\_LO\_LO\_RZ\_R$平行，矫正坐标系的$O\_{rec}Y\_{rec}$轴垂直该平面。
                

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/3216952e-c992-49d7-84e7-9ea1a5e51f80.png)

1.  根据i中关系求得矫正坐标系$x, y, z$三轴单位向量
    

$e\_1 = \frac{t}{||t||}$, $e\_2 = \frac{(0,0,1)^T ×  t}{||t×0,0,1)^T||}$, $e\_3 = \frac{e\_1 × e\_2}{e\_1 × e\_2}$

1.  矫正旋转矩阵
    

$R\_{rec} =  \begin{pmatrix} e\_1^T  \\\[1ex\] e\_2^T  \\\[1ex\] e\_3^T \end{pmatrix}$

**双目深度估计**

1.  最新发布论文目录
    

[https://vision.middlebury.edu/stereo/eval3/referenceList.php](https://vision.middlebury.edu/stereo/eval3/referenceList.php)

2.  主要步骤
    
    1.  **立体校正（Stereo Rectification）**：确保两个相机的图像平面对齐，并且其极线是平行的。这可以通过调用 `cv.stereoRectify()` 函数来完成。
        
    2.  **计算视差图（Disparity Map）**：利用两个图像之间的差异来估计深度信息。这可以通过使用Semi-Global Block Matching (SGBM) 或其他立体匹配算法来实现。在OpenCV中，你可以使用 `cv.StereoSGBM_create()` 函数来创建SGBM对象，然后调用 `compute()` 方法计算视差图。
        
    3.  **计算深度（Compute Depth）**：通过视差图计算深度。这可以通过将视差值与相机参数和基线长度进行关联来完成。例如，可以使用 `cv.reprojectImageTo3D()` 函数将视差图转换为三维点云。
        
    4.  **可视化深度图（Visualize Depth Map）**：将深度信息可视化，以便于理解和分析。可以使用OpenCV的绘图功能将深度图绘制成灰度图像或伪彩色图像，以表示不同深度的区域。
        
3.  OpenCV相关函数
    
    1.  [initUndistortRectifyMap()](https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a)
        
    2.  [stereoRectify()](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga617b1685d4059c6040827800e72ad2b6)
        
    3.  [remap()](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4)
        
    4.  [Camera Calibration and 3D Reconstruction](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga617b1685d4059c6040827800e72ad2b6)
        

## 三、编程与测试相关

**Gtest使用**

[https://blog.csdn.net/Jacksqh/article/details/129805462](https://blog.csdn.net/Jacksqh/article/details/129805462)

**C++中类模板头文件及源文件包含**

1.  头文件中应有类及函数的声明
    

```c++
#ifndef __EKF_H__
#define __EKF_H__

namespace Metabounds
{
  template <typename _Scalar>
  class EKF
  {
  public:
    explicit EKF(INSStates<_Scalar> InsStates);
  private:
    void test()l;
} // namespace Metabounds
```

2.  源文件中需要显性实例化类
    

```c++
#include "EKF.h"

namespace Metabounds
{
  template <typename _Scalar>
  EKF<_Scalar>::EKF(INSStates<_Scalar> InsStates)
  {
  }

template class Metabounds::EKF<double>;
template class Metabounds::EKF<float>;
} // namespace Metabounds
```

不显性实例化类会出现写面错误

```plaintext
/usr/bin/ld: CMakeFiles/imuiat_node_ins.dir/imuiat_node_ins.cpp.o: in function `Metabounds::Trajectory<double>::initTrjectory(double, Metabounds::Matrix<double, 3, 1> const&, Metabounds::Matrix<double, 3, 1> const&)':
imuiat_node_ins.cpp:(.text._ZN10Metabounds10TrajectoryIdE13initTrjectoryEdRKNS_6MatrixIdLi3ELi1EEES5_[_ZN10Metabounds10TrajectoryIdE13initTrjectoryEdRKNS_6MatrixIdLi3ELi1EEES5_]+0x1509): undefined reference to `Metabounds::EKF<double>::EKF(Metabounds::INSStates<double>)'
collect2: error: ld returned 1 exit status
make[2]: *** [test/CMakeFiles/imuiat_node_ins.dir/build.make:129：test/imuiat_node_ins] 错误 1
make[1]: *** [CMakeFiles/Makefile2:116：test/CMakeFiles/imuiat_node_ins.dir/all] 错误 2
make: *** [Makefile:91：all] 错误 2
```

3.  创建指针
    

```c++
std::unique_ptr<EKF<_Scalar>>     EKF_ptr_;
EKF_ptr_     = std::make_unique<EKF<_Scalar>>(InsStates_);
```

**需类型转换才能赋值**

1.  显示错误
    

```c++
error: conversion from ‘Metabounds::Block<Metabounds::Matrix<double, 18, 18>, 3, 3>’ to non-scalar type ‘Metabounds::Matrix<double, 3, 3>’ requested
   94 |     Matrix<_Scalar, 3, 3> test = Qc_.template block<ARW_ID, ARW_ID, 3, 3>();
```

2.  原定义
    

```c++
template <int StartRow, int StartCol, int Rows, int Cols>
const Block<Matrix<_Scalar, _Row, _Col>, Rows, Cols> block() const
{
  return Block<Matrix<_Scalar, _Row, _Col>, Rows, Cols>(*this, StartRow * _Col + StartCol);
}
```

3.  解决方案
    
    1.  先定义`Matrix<_Scalar, 3, 3> test;`
        
    2.  再赋值`test = Qc_.template block<ARW_ID, ARW_ID, 3, 3>();`
        
4.   或者添加能直接返回所需类型的函数
    

```c++
// 提取指定维度的子矩阵
template <typename _Scalar0, int Rows, int Cols>
Matrix<_Scalar0, Rows, Cols> block(int startRow, int startCol)
{
  // 检查子矩阵是否在原始矩阵范围内
  if (startRow + Rows > _Row || startCol + Cols > _Col)
  {
    throw std::out_of_range("Block is out of bounds");
  }

  // 创建新的子矩阵
  Matrix<_Scalar, Rows, Cols> subMatrix;

  // 填充子矩阵
  for (int i = 0; i < Rows; ++i)
  {
    for (int j = 0; j < Cols; ++j)
    {
      subMatrix(i, j) = (*this)(startRow + i, startCol + j); 
    }
  }
  return subMatrix;
}
```

*   使用方法：`Matrix<_Scalar, 3, 3> test = Qc_.template block<3, 3>(ARW_ID, ARW_ID);`
    

**在ubuntu与windows共享文件夹中编译keil工程报错**

1.  找不到多个文件
    
2.  原因：在virtual box中文件路径会有问题
    
3.  解决：将工程放到windows系统文件夹中
    

## 四、信号处理及理论相关

[**卡尔曼滤波**](https://zh.wikipedia.org/wiki/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2) **Kalman filter**

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/bb380189-be3c-40ee-aeb8-4ffad97f1953.png)

1.  定义：一种高效率的递归滤波器（自回归滤波器），它能够从一系列的不完全及包含噪声的测量中，估计动态系统的状态。会根据各测量量在不同时间下的值，考虑**各时间下的联合分布**，再产生对未知变数的估计，因此会比只以单一测量量为基础的估计方式要准。
    
2.  算法步骤
    
    1.  **预测**：使用上一状态的估计，做出对当前状态的估计，其中也包括不确定性。
        
    2.  **更新**：利用对当前状态的观测值优化在预测阶段获得的预测值，以获得一个更精确的新估计值。通过**加权平均**来更新估计值，而确定性越高的量测加权比重也越高。
        
    3.  需要：输入量测，以往的计算值以及其不确定性矩阵。
        
3.  基本动态系统模型
    
    1.  基础：建立在线性代数和隐马尔可夫模型（Hidden Markov Model）上；
        
    2.  可以用一个马尔可夫连链表示：
        
        1.  建立在一个被**高斯噪声**（正态分布的噪声）干扰的线性算子上；
            
        2.  系统的状态可以**用一个元素为实数的向量**表示；
            
        3.  随着时间增加，这个线性算子会作用在当前状态上，**产生一个新状态**，并**带入一些噪声**，同时系统的信息也会被加入；
            
        4.  另一个受噪声干扰的线性算子产生出这些隐含状态的**可见输出**。
            
4.  在卡尔曼滤波的框架下建立模型：
    
    1.  ![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/NpQlKav4YE2DqDvL/img/aa5fa6c0-ff22-4ce8-b9ea-379e9bb9fa0e.png)
        
    2.  对于每一步_k_，定义矩阵$\textbf{F}\_k$, $\textbf{H}\_k$, $\textbf{Q}\_k$, $\textbf{R}\_k$，有时也需要定义$\textbf{B}\_k$，如下。
        
    3.  卡尔曼滤波模型假设_k_时刻的真实状态是从（_k_ − 1）时刻的状态演化而来，符合下式：
        

$\mathbf{x}\_k=\mathbf{F}\_k \mathbf{x}\_{k−1}+\mathbf{B}\_k \mathbf{u}\_k+\mathbf{w}\_k$

其中

*   $\textbf{F}\_k$是作用在$\textbf{x}\_{k-1}$上的**状态变换模型（/矩阵/向量）**。
    
    *   $\textbf{B}\_k$是作用在控制器向量$\textbf{u}\_k$上的输入－**控制模型**。
        
    *   $\textbf{w}\_k$是**过程噪声**，并假定其符合均值为零，**协方差矩阵为**$\textbf{Q}\_k$的多元正态分布。
        

$\mathbf{w}\_k∼N(0,\mathbf{Q}\_k)$

时刻_k_，对**真实状态**$\textbf{x}\_k$的一个**测量**$\textbf{z}\_k$满足下式：

$\mathbf{z}\_k=\mathbf{H}\_k \mathbf{x}\_k+\mathbf{v}\_k$

其中$\textbf{H}\_k$是**观测模型**，它把真实状态空间映射成观测空间，$\textbf{v}\_k$**是观测噪声**，其均值为零，协方差矩阵为$\textbf{R}\_k$，且服从正态分布。

$\mathbf{v}\_k∼N(0,\mathbf{R}\_k)$

初始状态以及每一时刻的噪声{**x**~0~, **w**~1~, ..., **w**~_k_~,  **v**~1~ ... **v**~_k_~}都认为是互相独立的。

5.  卡尔曼滤波器
    
    1.  表示状态的两个变量：
        
        1.  $\mathbf{\hat  x}\_{k|k}$，在时刻_k_的状态的估计；
            
        2.  $\mathbf{P}\_{k|k}$，后验估计误差协方差矩阵，度量估计值的精确程度。
            
    2.  **预测**
        

${\displaystyle {\hat {\textbf {x}}}\_{k|k-1}={\textbf {F}}\_{k}{\hat {\textbf {x}}}\_{k-1|k-1}+{\textbf {B}}\_{k}{\textbf {u}}\_{k}}$（预测状态）

$\mathbf{P}\_{k|k−1}= \mathbf{F}\_k  \mathbf{P}\_{k−1|k−1} \mathbf{F}\_k^T +\mathbf{Q}\_k$（预测估计协方差矩阵）

*   没有控制时，可忽略$\textbf{B}\_k$和$\textbf{u}\_k$。
    

1.  **更新**
    
    1.  首先算出以下三个量：
        

${\displaystyle {\tilde {\textbf {y}}}\_{k}={\textbf {z}}\_{k}-{\textbf {H}}\_{k}{\hat {\textbf {x}}}\_{k|k-1}}$（测量残差）

${\displaystyle {\textbf {S}}\_{k}={\textbf {H}}\_{k}{\textbf {P}}\_{k|k-1}{\textbf {H}}\_{k}^{T}+{\textbf {R}}\_{k}}$（测量残差协方差）

${\displaystyle {\textbf {K}}\_{k}={\textbf {P}}\_{k|k-1}{\textbf {H}}\_{k}^{T}{\textbf {S}}\_{k}^{-1}}$（最优卡尔曼增益）

2.  然后用它们来更新滤波器变量$\textbf x$与$\textbf P$：
    

${\displaystyle {\hat {\textbf {x}}}\_{k|k}={\hat {\textbf {x}}}\_{k|k-1}+{\textbf {K}}\_{k}{\tilde {\textbf {y}}}\_{k}}$（更新的状态估计）

${\displaystyle {\textbf {P}}\_{k|k}=(\textbf {I}-{\textbf {K}}\_{k}{\textbf {H}}\_{k}){\textbf {P}}\_{k|k-1}}$（更新的协方差估计）

*   使用上述公式计算$\textbf{P}\_{k|k}$仅在最优卡尔曼增益的时候有效。
    

1.  **不变量（Invariant）**
    

如果模型准确，而且${\displaystyle {\hat {\textbf {x}}}\_{0|0}}$与${\displaystyle {\textbf {P}}\_{0|0}}$的值准确地反映了最初状态的分布，那么以下不变量就保持不变：所有估计的误差均值为零

${\displaystyle {\textrm {E}}\[{\textbf {x}}\_{k}-{\hat {\textbf {x}}}\_{k|k}\]={\textrm {E}}\[{\textbf {x}}\_{k}-{\hat {\textbf {x}}}\_{k|k-1}\]=0}$

${\displaystyle {\textrm {E}}\[{\tilde {\textbf {y}}}\_{k}\]=0}$

且协方差矩阵准确地反映了估计的协方差：

${\displaystyle {\textbf {P}}\_{k|k}={\textrm {cov}}({\textbf {x}}\_{k}-{\hat {\textbf {x}}}\_{k|k})}$

${\displaystyle {\textbf {P}}\_{k|k-1}={\textrm {cov}}({\textbf {x}}\_{k}-{\hat {\textbf {x}}}\_{k|k-1})}$

${\displaystyle {\textbf {S}}\_{k}={\textrm {cov}}({\tilde {\textbf {y}}}\_{k})}$

请注意，其中${\displaystyle {\textrm {E}}\[{\textbf {a}}\]}$表示${\displaystyle {a}}$的期望值, ${\displaystyle {\textrm {cov}}({\textbf {a}})={\textrm {E}}\[{\textbf {a}}{\textbf {a}}^{T}\]}$。 

6.  应用方法与步骤
    
    1.  构造了连续时间**状态变换模型（/矩阵/向量）**$\textbf{F}$，离散化得到状态转移矩阵