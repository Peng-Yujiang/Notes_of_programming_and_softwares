{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book-Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition by Aurélien Géron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a note of the book ***Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition***\n",
    "\n",
    "Author: Yujiang Peng\n",
    "\n",
    "Date: Feb 12, 2020\n",
    "\n",
    "##### 1. Date manipulation\n",
    "* 1.1.载入数据、保存、查看\n",
    "* 1.2.数据复制、分类、保存、组合、删除\n",
    "\n",
    "##### 2.Certain functions\n",
    "* 2.0.某些功能\n",
    "* 2.1.下载、获取、保存、操作文件\n",
    "* 2.2.\n",
    "\n",
    "##### 3.Plot\n",
    "* 3.1.柱状图\n",
    "* 3.2.线图及散点图\n",
    "* 3.\n",
    "\n",
    "##### 4.Machine learning\n",
    "* 4.1.数据预处理\n",
    "  * 4.1.1.拆分数据集\n",
    "  * 4.1.2.各列数据之间的相关性\n",
    "  * 4.1.3.数据预处理流程\n",
    "* 4.2.模型选择与预测\n",
    "* 4.3.调整模型/调参\n",
    "* 4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Date manipulation\n",
    "* 1.1.载入数据、保存、查看\n",
    "* 1.2.数据复制、分类、保存、组合、删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 NumPy\n",
    "\n",
    "* NumPy Arrays, dtype, and shape\n",
    "* Common Array Operations\n",
    "* Reshape and Update In-Place\n",
    "* Combine Arrays\n",
    "* Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前缀r表示原始字符串，在输入输出时非常实用\n",
    "print(r'C:\\nowhere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#将目录和文件名合成一个路径\n",
    "datapath = os.path.join(\"FolderName\", \"LowerFolderName\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.载入数据、保存、查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "Data_1 = pd.read_csv(datapath + \"FileName.csv\",thousands=',',delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "# 或采用下列函数，可根据文件类型改写\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"FileName.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存到文件\n",
    "Data_1.to_csv(os.path.join(\"FolderName\", \"LowerFolderName\", \"FileName.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看前几行数据\n",
    "Data_1.head(2)\n",
    "Data_1.[\"ColumnName\"].head()\n",
    "# 查看数据列名称、数量、数据类型、文件大小\n",
    "Data_1.info()\n",
    "# 类别类型的数量\n",
    "Data_1[\"ColumnName\"].value_counts()\n",
    "# 各列数据数量、均值、标准差、最大最小值、各分位数值\n",
    "Data_1.describe()\n",
    "# 查看数据维度\n",
    "Data_1.shape\n",
    "# 查看数据类型\n",
    "Data_1.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.数据复制、分类、保存、组合、删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度复制数据\n",
    "Data_1 = Data_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将某列数据划分类别\n",
    "Data_1[\"NewColumnName\"] = pd.cut(Data_1[\"ColumnName\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两列数据之间操作，并保存在新列中\n",
    "Data_1[\"NewColumnName\"] = Data_1[\"ColumnName_1\"]/Data_1[\"ColumnName_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将单独各列数据根据索引组成在一起\n",
    "DataFrame_1 = pd.DataFrame({\n",
    "    \"ColumnName_1\": Data_1,\n",
    "    \"ColumnName_2\": Data_2,\n",
    "}).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除某行或某列数据\n",
    "# note that drop() creates a copy of the data and does not affect set_\n",
    "# drop() creates a copy of the data and does not affect strat_train_set\n",
    "for set_ in (DataSet_1, DataSet_2):\n",
    "    set_.drop(\"ColumnName\", axis=1, inplace=True) # axis=1表示某列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出有缺失值的数据，部分显示\n",
    "sample_incomplete_rows = Data_1[Data_1.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.c_和np.r_\n",
    "# np.r_是按列连接两个矩阵，就是把两矩阵上下相加，要求列数相等。\n",
    "# np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等\n",
    "np.c_[Data_1, Data_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改变数据形状，选择新的索引\n",
    "Data_1 = Data_1.pivot(index=\"ColumnName\", columns=\"IndicatorColumnName\",\\\n",
    "                      values=\"Value\")\n",
    "# 重命名列\n",
    "Data_2.rename(columns={\"OriginalColumnName\": \"NewColumnName\"}, inplace=True)\n",
    "# 添加索引\"index\"\n",
    "Data_1.reset_index()\n",
    "# 更换索引\n",
    "Data_1.set_index(\"ColumnName\", inplace=True)\n",
    "# 合并数据\n",
    "Data_3 = pd.merge(left=Data_1, right=Data_2,\n",
    "                                  left_index=True, right_index=True)\n",
    "Data_3.sort_values(by=\"ColumnName\", inplace=True)\n",
    "# 保留或删除特定数据\n",
    "remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "# .iloc根据索引index数值查找数据\n",
    "Data_3[[\"ColumnName_1\", 'ColumnName_2']].iloc[keep_indices]\n",
    "# .loc根据索引index名称查找数据\n",
    "Data_3[[\"ColumnName_1\", 'ColumnName_2']].loc[\"RowName\"]\n",
    "Data_3.loc[\"RowName\"][\"ColumnName\"]\n",
    "Data_3.loc[[c for c in Data_.index if \"W\" in c.upper()]][\"ColumnName\"]\n",
    "# 根据字典键值keys查找数据\n",
    "Data_1.loc[list(Dict_1.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Certain functions\n",
    "* 2.0.某些功能\n",
    "* 2.1.下载、获取、保存、操作文件\n",
    "* 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.某些功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.下载、获取、保存、操作文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"FolderName\", \"LowerFolderName\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"FolderName/LowerFolderName/FileName.csv\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Plot\n",
    "\n",
    "* 3.1.柱状图\n",
    "* 3.2.线图及散点图\n",
    "* 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.柱状图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别绘制各列数据的柱状图\n",
    "%matplotlib inline\n",
    "Data_1.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据某列数据绘制柱状图\n",
    "Data_1[\"ColumnName\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.线图及散点图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "# 这里的x和y是所绘图的坐标轴名称\n",
    "ax = Data_1.plot(kind='scatter', x=\"1st_ColumnName\", y='2ed_ColumnName',\\\n",
    "                 figsize=(5,3), alpha=0.1,\\\n",
    "           # 将第三列数据作为标识颜色的依据，并在右边绘制彩色标尺\n",
    "            # s代表散点半径，由第三列数据确定\n",
    "            s=Data_1[\"3rd_ColumnName\"]/100, label=\"ZLabel\", \\\n",
    "            # c代表散点颜色，由第四列数据确定\n",
    "            c=\"4th_ColumnName-ScaleBarLabel\", cmap=plt.get_cmap(\"jet\"),\\\n",
    "                 colorbar=True, sharex=False)\n",
    "# 插入图片作为背景\n",
    "import matplotlib.image as mpimg\n",
    "Insert_img=mpimg.imread(os.path.join(images_path, filename))\n",
    "plt.imshow(Insert_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "           cmap=plt.get_cmap(\"jet\"))\n",
    "Data_4th_Column = Data_1[\"4th_ColumnName\"]\n",
    "tick_values = np.linspace(Data_4th_Column.min(),Data_4th_Column.max(),Step_ScaleBar)\n",
    "cbar = plt.colorbar(ticks=tick_values/prices.max())\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "cbar.set_label('ScaleBarLabel', fontsize=16)\n",
    "\n",
    "# 设置轴线数据显示范围\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "# 绘制参考线（根据方程绘制）\n",
    "X=np.linspace(X_Start, X_end, X_Step)\n",
    "plt.plot(X, f(X), \"r--\", label=\"LabelText\")\n",
    "# 绘制垂直于坐标轴的参考线\n",
    "plt.plot([X_Start, X_Start], [0, Y_end], \"r--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加文本，可添加多行\n",
    "# theta_0表示有下标0\n",
    "plt.text(X_Text_Position, Y_Text_Position, r\"$\\theta_0 = 0$\", fontsize=14, color=\"r\")\n",
    "# 显示图例\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "# 设置坐标轴名称标签，默认使用列名\n",
    "plt.xlabel(\"XAxisLabel\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加指向具体数据点的箭头和文字\n",
    "position_text = {\n",
    "    \"RowName_1\": (X_Data_1, Y_Data_1),\n",
    "    \"RowName_2\": (X_Data_2, Y_Data_2),\n",
    "}\n",
    "# Data_1应有两列数据\n",
    "for Text_, X-Y_Data_ in Data_1.items():\n",
    "    pos_data_x, pos_data_y = sample_data.loc[RowName_]\n",
    "    # 更换文本\n",
    "    RowName_ = \"NewText_\" if country == \"Text_x\" else RowName_\n",
    "    plt.annotate(country, xy=(pos_data_x, pos_data_y), xytext=pos_text,\n",
    "            arrowprops=dict(facecolor='black', width=0.5, shrink=0.1, headwidth=5))\n",
    "    # 将这些点绘制为红色\n",
    "    plt.plot(pos_data_x, pos_data_y, \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建函数来保存图片，设置保存位置\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"fundamentals\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "# 保存图片，显示图片\n",
    "save_fig('money_happy_scatterplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Machine learning\n",
    "* 4.1.数据预处理\n",
    "  * 4.1.1.拆分数据集\n",
    "  * 4.1.2.各列数据之间的相关性\n",
    "  * 4.1.3.数据预处理流程\n",
    "* 4.2.模型选择与预测\n",
    "* 4.3.调整模型/调参\n",
    "* 4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1.拆分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn函数拆分数据，随机拆分\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(Data_1, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按各类数量等比例拆分\n",
    "# 1. 先将某列数据划分类别\n",
    "Data_1[\"ColumnName\"] = pd.cut(Data_1[\"OldColumnName\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "# 2. 再在各个类别中分别拆分数据集\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(Data_1, Data_1[\"ColumnName\"]):\n",
    "    strat_train_set = Data_1.loc[train_index]\n",
    "    strat_test_set = Data_1.loc[test_index]\n",
    "# 3. 计算验证集中各类别比例  \n",
    "strat_test_set[\"ColumnName\"].value_counts() / len(strat_test_set)\n",
    "# 4. 计算数据集中各类别比例\n",
    "Data_1[\"ColumnName\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration only. Sklearn has train_test_split()\n",
    "# 直接整块拆分\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "import hashlib\n",
    "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
    "    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n",
    "    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n",
    "\n",
    "# 自定义数据拆分方法\n",
    "# 根据id拆分数据\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "# 使用一：将索引作为id\n",
    "housing_with_id = Data_1.reset_index()   # adds an `index` column\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "# 使用二：id与具体数据关联\n",
    "housing_with_id[\"id\"] = Data_1[\"ColumnName_1\"] * 1000 + Data_1[\"ColumnName_2\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2.各列数据之间的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各列属性之间的标准相关系数\n",
    "# the standard correlation coefficient (also called Pearson’s r)\n",
    "corr_matrix = Data_1.corr()\n",
    "# 各列属性与某一列属性之间的相关系数\n",
    "corr_matrix[\"ColumnName\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另一种检查属性之间相关性的方法\n",
    "# 绘制各列属性之间的散点图\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"1st_ColumnName\", \"2ed_ColumnName\", \"3ed_ColumnName\"]\n",
    "scatter_matrix(Data_1[attributes], figsize=(12, 8))\n",
    "# 也可单独绘制两列属性数据的散点图\n",
    "Data_1.plot(kind=\"scatter\", x=\"ColumnName_1\", y=\"ColumnName_2\", alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3数据预处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 通过再次复制训练集恢复到清洗后的训练数据集，同时分离预测值与实际标签，\n",
    "#    去掉训练集的实际标签\n",
    "Data_1 = strat_train_set.drop(\"ColumnName_Label\", axis=1)\n",
    "Data_1_labels = strat_train_set[\"ColumnName_Label\"].copy()\n",
    "# 1. 数据清洗，data cleaning\n",
    "#  1.1 找出有缺失值的属性\n",
    "sample_incomplete_rows = Data_1[Data_1.isnull().any(axis=1)].head()\n",
    "#  1.2 处理有缺失值的数值类属性\n",
    "#    Option 1：直接删除有缺失值样本\n",
    "sample_incomplete_rows.dropna(subset=[\"ColumnName_Incomplete\"])\n",
    "#    Option 2：直接删除有缺失值样本的属性\n",
    "sample_incomplete_rows.drop(\"ColumnName_Incomplete\", axis=1)\n",
    "#    Option 3：用0、均值、中值等填充样本缺失值\n",
    "median = Data_1[\"ColumnName_Incomplete\"].median()\n",
    "sample_incomplete_rows[\"ColumnName_Incomplete\"].fillna(median, inplace=True)\n",
    "#    Option 4：使用sklearn的imput()函数处理\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "#    去除非数值属性\n",
    "Data_1_num = Data_1.drop(\"ColumnName_Inc_NonNumerical\", axis=1)\n",
    "# alternatively: Data_1_num = Data_1.select_dtypes(include=[np.number])\n",
    "#    fit the imputer instances\n",
    "imputer.fit(Data_1_num)\n",
    "#    imputer computed the median of each attribute and stored the result\n",
    "#    in its statistics_ instance variable.\n",
    "#    查看具有缺失值属性的计算结果\n",
    "imputer.statistics_\n",
    "Data_1_num.median().values # 另一种方法计算的中值\n",
    "#    用由imput计算得到的结果代替缺失值\n",
    "X = imputer.transform(Data_1_num) # 结果为Numpy数组\n",
    "#    转换为pandas的DataFrame\n",
    "Data_1_tr = pd.DataFrame(X, columns=Data_1_num.columns, index=Data_1_num.index)\n",
    "#    查看转换后的具有缺失值的样本\n",
    "Data_1_tr.loc[sample_incomplete_rows.index.values]\n",
    "\n",
    "#  1.3 处理有缺失值的类别性属性\n",
    "Data_1_cat = Data_1[[\"ColumnName_Inc_Categorical\"]]\n",
    "#   将类别转换为数值\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "Data_1_cat_encoded = ordinal_encoder.fit_transform(Data_1_cat)\n",
    "#    查看类别\n",
    "ordinal_encoder.categories_\n",
    "#   或者转换为onehot metrix，更好；结果为SciPy稀疏矩阵\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "Data_1_cat_1hot = cat_encoder.fit_transform(Data_1_cat)\n",
    "#    可将稀疏矩阵转换为密矩阵\n",
    "Data_1_cat_1hot.toarray()\n",
    "#   或者在创建OneHotEncoder时使用sparse=False参数\n",
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "Data_1_cat_1hot = cat_encoder.fit_transform(Data_1_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 定制transformer\n",
    "#    功能：可执行cleanup操作或者结合某些特定的属性\n",
    "#    步骤：创建一个类，再执行三个方法fit() (returning self), transform(), fit_transform()\n",
    "#    \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# 将TransformerMixin作为基类时不需要执行fit_transform()\n",
    "# 将BaseEstimator作为基类且无带*参数时将获得两个额外的方法:get_params(),set_params()\n",
    "# 添加组合属性\n",
    "ColumnIndex_1, ColumnIndex_2, ColumnIndex_3 = 3,4,5 # indice为列序数\n",
    "#  添加组合的属性\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_ColumnName_ToAdd = True): # no *args or **kargs\n",
    "        # 这段程序中transformer超参数为add_ColumnName_ToAdd\n",
    "        self.ColumnName_ToAdd = ColumnName_ToAdd\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        New_ColumnName = X[:, ColumnIndex_3] / X[:, ColumnIndex_4]\n",
    "        if self.add_ColumnName_ToAdd:\n",
    "            ColumnName_ToAdd = X[:, ColumnIndex_1] / X[:, ColumnIndex_2]\n",
    "            return np.c_[X, New_ColumnName, ColumnName_ToAdd]\n",
    "        else:\n",
    "            return np.c_[X, New_ColumnName]\n",
    "attr_adder = CombinedAttributesAdder(add_ColumnName_ToAdd=False)\n",
    "Data_1_extra_attribs = attr_adder.transform(Data_1.values) # Numpy array\n",
    "# 可通过下列代码动态地获取columnIndex\n",
    "col_names = \"ColumnName_1\", \"ColumnName_2\", \"ColumnName_3\"\n",
    "ColumnIndex_1, ColumnIndex_2, ColumnIndex_3 = [\n",
    "    Data_1.columns.get_loc(c) for c in col_names] # get the column indices\n",
    "# 将Numpy array恢复为DataFrame\n",
    "Data_1_extra_attribs = pd.DataFrame(\n",
    "    Data_1_extra_attribs,\n",
    "    columns=list(Data_1.columns)+[\"ColumnName_1\", \"ColumnName_2\"],\n",
    "    index=Data_1.index)\n",
    "Data_1_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature scaling\n",
    "# 两种常用的方法：min-max scaling; standardization\n",
    "MinMaxScaler # 归一化，值为0-1；feature_range可更改范围\n",
    "standardization # outliers离群值影响较小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transformation Pipelines\n",
    "# pipeline for the numerical attributes\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ]) # 除最后一个外必须是transformer，即必须有fit_transform()方法\n",
    "Data_1_num_tr = num_pipeline.fit_transform(Data_1_num)\n",
    "# 一个同时适用于数字列和类别列的transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs = list(Data_1_num)# 获取各数值列名\n",
    "cat_attribs = [\"ColumnName_Categorical\"] # 获取各类别列名\n",
    "full_pipeline = ColumnTransformer([ # requires a list of tuples\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ]) # a name, a transformer, and a list of names (or indices) of columns\n",
    "Data_1_prepared = full_pipeline.fit_transform(Data_1)\n",
    "# drop去掉某列，passthrough不处理某列，若需其它不同的处理可设置remainder超参数\n",
    "\n",
    "# A full pipeline with both preparation and prediction\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", full_pipeline),\n",
    "        (\"linear\", LinearRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.模型选择与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全流程实例\n",
    "import sklearn.linear_model\n",
    "# Select a model, 有上述import操作后可不加sklearn.\n",
    "ModelName_1 = sklearn.linear_model.LinearRegression() # 线性回归\n",
    "ModelName_2 = sklearn.linear_model.Ridge(alpha=10**9.5) #\n",
    "ModelName_3 = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3) # KNN\n",
    "ModelName_4 = DecisionTreeRegressor(random_state=42) # 决策树\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "ModelName_5 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "from sklearn.svm import SVR\n",
    "ModelName_6 = SVR(kernel=\"linear\")\n",
    "# 数据预处理\n",
    "Data_1_prepared = full_pipeline.transform(Data_1)\n",
    "# Train the model\n",
    "ModelName_.fit(X_Data_1, Y_Data_1)\n",
    "# 模型参数（线性）\n",
    "t0, t1 = ModelName_.intercept_[0], ModelName_.coef_[0][0]\n",
    "# Make a prediction\n",
    "X_new = [[22587]]  # X value to predict\n",
    "ModelName_.predict(X_new) # predict\n",
    "ModelName_.predict([[X_Value]])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=60, include_bias=False)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "\n",
    "pipeline_reg = pipeline.Pipeline([('poly',poly),('scal',scaler),('lin',lin_reg)])\n",
    "pipeline_reg.fit(Xfull, yfull)\n",
    "curve = pipeline_reg.predict(X[:, np.newaxis])\n",
    "plt.plot(X, curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集的训练与评估\n",
    "# 计算预测结果标准差\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lin_mse = mean_squared_error(Data_labels, Data_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "# 计算预测结果绝对误差\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "lin_mae = mean_absolute_error(Data_labels, Data_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更好的评估方法：k折交叉验证\n",
    "#  将训练集拆分为更小的训练集和验证集，使用train_test_split()函数\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, Data_prepared, Data_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.调整模型/调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 几种调参方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 搜索中各超参数组合的分数\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "#  将分数转换为DataFrame\n",
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.网格搜索 Grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(Data_prepared, Data_labels)\n",
    "# 查看最优参数组合\n",
    "grid_search.best_params_\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 随机搜索 Randomized Search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(Data_prepared, Data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 组合方法\n",
    "#  通过组合来调整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 分析最优模型及其误差\n",
    "#  各属性的相对重要性\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "#  重要性分数\n",
    "extra_attribs = [\"ColumnName_1\", \"ColumnName_2\", \"ColumnName_3\"]\n",
    "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 在验证集上评估模型\n",
    "final_model = grid_search.best_estimator_\n",
    "X_test = strat_test_set.drop(\"ColumnName_toPredict\", axis=1)\n",
    "y_test = strat_test_set[\"ColumnName_toPredict\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "# 计算t检验95%置信区间\n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                         loc=squared_errors.mean(),\n",
    "                         scale=stats.sem(squared_errors)))\n",
    "# 手动计算t检验95%置信区间\n",
    "m = len(squared_errors)\n",
    "mean = squared_errors.mean()\n",
    "tscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\n",
    "tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\n",
    "np.sqrt(mean - tmargin), np.sqrt(mean + tmargin)\n",
    "\n",
    "# 使用z检验\n",
    "zscore = stats.norm.ppf((1 + confidence) / 2)\n",
    "zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\n",
    "np.sqrt(mean - zmargin), np.sqrt(mean + zmargin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.分类\n",
    "* 5.1. 二元分类\n",
    "* 5.2. 性能度量\n",
    "  * 5.2.1. 通过交叉验证测量准确率\n",
    "  * 5.2.2. 混淆矩阵confusion matrix\n",
    "  * 5.2.3. 准确度和recall\n",
    "  * 5.2.4. 权衡准确度和recall\n",
    "  * 5.2.5. ROC曲线\n",
    "* 5.3. 多元分类\n",
    "* 5.4. 误差分析\n",
    "* 5.5. 多标签分类\n",
    "* 5.6. 多输出分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. 二元分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (SGD)分类器\n",
    "# 优点：可高效处理大数据集\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "\n",
    "# 预测\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. 性能度量（二元分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过交叉验证测量准确率\n",
    "# 进行k折检验\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "### 准确率通常并不是分类器理想的性能度量，尤其是处理偏斜样本集时skewed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混淆矩阵confusion matrix\n",
    "\n",
    "# 生成步骤\n",
    "# 1. 进行k折检验，返回每折验证集上的预测值\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "# 2. 获得混淆矩阵\n",
    "#     其中，每行代表实际类别，每列代表预测得到的类别\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确度和recall\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "# 计算准确度\n",
    "precision_score(y_train_5, y_train_pred)\n",
    "# 计算recall\n",
    "recall_score(y_train_5, y_train_pred)\n",
    "\n",
    "# 计算F_1分数，即准确度与recall的调和平均值\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权衡准确度和recall\n",
    "\n",
    "# 1.计算分数\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")\n",
    "# 2.计算所有阈值下对应的准确度与recall\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "# 3.绘制准确度、recall与阈值的函数图像，或准确度与recall的函数图像\n",
    "\n",
    "# 计算准确度大于0.9时的recall\n",
    "recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
    "# 计算准确度大于0.9时的阈值\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "### 绘制准确度、recall与阈值的函数图像\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.grid(True)                              # Not shown\n",
    "    plt.axis([-50000, 50000, 0, 1])             # Not shown\n",
    "plt.figure(figsize=(8, 4))                                                                  # Not shown\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")# Not shown\n",
    "plt.plot([threshold_90_precision], [0.9], \"ro\")                                             # Not shown\n",
    "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             # Not shown\n",
    "save_fig(\"precision_recall_vs_threshold_plot\")                                              # Not shown\n",
    "plt.show()\n",
    "\n",
    "### 绘制准确度与recall的函数图像\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], \"r:\")\n",
    "plt.plot([0.0, recall_90_precision], [0.9, 0.9], \"r:\")\n",
    "plt.plot([recall_90_precision], [0.9], \"ro\")\n",
    "save_fig(\"precision_vs_recall_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC曲线\n",
    "\n",
    "# 1.比较二元分类器的第一种方法\n",
    "## 计算各种不同阈值下的TPR和FPR值\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "## 绘制FPR与TPR的函数图像\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "plt.figure(figsize=(8, 6))                                    # Not shown\n",
    "plot_roc_curve(fpr, tpr)\n",
    "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]           # Not shown\n",
    "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")   # Not shown\n",
    "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")  # Not shown\n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")               # Not shown\n",
    "save_fig(\"roc_curve_plot\")                                    # Not shown\n",
    "plt.show()\n",
    "\n",
    "# 2.比较分类器的另一种方法\n",
    "## 测量曲线下的面积\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)\n",
    "\n",
    "## 计算随机森林分类器的ROC曲线和ROC AUC分数\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "                                    method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. 多元分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. 多标签分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5. 误差分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6. 多输出分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "53a1c45876d37b93742cefcbccd0e028309dbdecc6e457f55650f19fbe3f8ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
